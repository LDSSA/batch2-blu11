{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLU11 - Learning Notebook - Part 3 of 3 - Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Collaborative-Filtering RS\n",
    "\n",
    "Collaborative-filtering (RS, henceforth) is another type of memory-based RS.\n",
    "\n",
    "The central premise of this type of recommenders is that if one has enough Ratings, then no other information is necessary.\n",
    "\n",
    "So, to be clear, this is what you need. (A lot of it, though.)\n",
    "\n",
    "![collaborative_filtering](../media/recommender_systems_framework_collaborative_filtering.png)\n",
    "\n",
    "This idea couldn't be more different from most approaches we've studied, where exploring and extracting features was front and center.\n",
    "\n",
    "In CF, a typical pipeline goes as follows, and we can think of it as filling in the blank spaces in our Ratings.\n",
    "\n",
    "![collaborative_filtering](../media/collaborative_filtering.png)\n",
    "\n",
    "We start by computing similarities between users or items; then we select the best neighbors.\n",
    "\n",
    "Then, we predict the user preference as a weighted average of the neighbors and filter the best results.\n",
    "\n",
    "## 1.1 Ratings\n",
    "\n",
    "As we've seen, Ratings are at the core of CF. \n",
    "\n",
    "As usual, the first steps is to build our User-Item matrix $R$, containing the available Ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ratings():\n",
    "    path = os.path.join('..', 'data', 'ml-latest-small', 'ratings.csv')\n",
    "    \n",
    "    users = np.genfromtxt(path, dtype='int', skip_header=True, usecols=[0], delimiter=',')\n",
    "    movies = np.genfromtxt(path, dtype='int', skip_header=True, usecols=[1], delimiter=',')\n",
    "    ratings = np.genfromtxt(path, skip_header=True, usecols=[2], delimiter=',')\n",
    "    \n",
    "    return users, movies, ratings\n",
    "\n",
    "\n",
    "users, movies, ratings = read_ratings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reuse the funtions from the last notebook to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ratings(users, movies, ratings):\n",
    "    \n",
    "    cols = movies - 1\n",
    "    rows = users - 1\n",
    "    \n",
    "    nrows = rows.max() + 1\n",
    "    ncols = cols.max() + 1\n",
    "    shape = (nrows, ncols)\n",
    "    \n",
    "    data = ratings\n",
    "    \n",
    "    coo = coo_matrix((data, (rows, cols)), shape=shape)\n",
    "    \n",
    "    return coo.tocsr()\n",
    "\n",
    "    \n",
    "R = make_ratings(users, movies, ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Compute Similarities\n",
    "\n",
    "As in content-based filtering, the default similarity measure is the cosine distance.\n",
    "\n",
    "What is different are the vectors we want to compare. We want the distance between User or Item vectors,  or $r_j$ respectively.\n",
    "\n",
    "There are two main types of CF, user-user and item-item filtering.\n",
    "\n",
    "In user-user CF, we compute how similar the users, i.e., the row-vectors $r^T$ of the ratings matrix $R \\in \\mathbb{R}^{\\space m \\space \\times \\space n}$, are to each other.\n",
    "\n",
    "Item-item approaches care about the similarity across all pairs of items, i.e., the columns-vectors $r$ in the column matrix.\n",
    "\n",
    "## 2.1 User-User CF\n",
    "\n",
    "To best understand this type of RS we use the tagline \"users who are similar to you also liked\".\n",
    "\n",
    "We compute similarities across all possible pairs of rows $(u, v) \\in U \\times U$. \n",
    "\n",
    "$$\\begin{bmatrix}1 & cosine(u_0, u_1) & ... & cosine(u_0, u_m) \\\\ cosine(u_1, u_0) & 1 & ... & cosine(u_1, u_m) \\\\ ...  & ... & ... & ...\\\\ cosine(u_m, u_0) & cosine(u_m, u_1) & ... & 1\\end{bmatrix}$$\n",
    "\n",
    "Regardless of the similarity metric used, the result is a matrix in $\\mathbb{R}^{\\space m \\space \\times \\space m}$. \n",
    "\n",
    "Again, we use the cosine from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_user_similarities(R):\n",
    "    return cosine_similarity(R, dense_output=False)\n",
    "\n",
    "\n",
    "user_similarities = make_user_similarities(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Item-Item CF\n",
    "\n",
    "Alternatively, the tagline for item-item CF is \"users who liked this item also liked\".\n",
    "\n",
    "We similarities across columns $(i, j) \\in I \\times I$. \n",
    "\n",
    "$$\\begin{bmatrix}1 & cosine(i_0, i_1) & ... & cosine(i_0, i_n) \\\\ cosine(i_1, i_0) & 1 & ... & cosine(i_1, i_n) \\\\ ...  & ... & ... & ...\\\\ cosine(i_n, i_0) & cosine(i_n, i_1) & ... & 1\\end{bmatrix}$$\n",
    "\n",
    "The result is a matrix in $\\mathbb{R}^{\\space n \\space \\times \\space n}$. (A rather big one.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_item_similarities(R):\n",
    "    return cosine_similarity(R.T, dense_output=False)\n",
    "\n",
    "\n",
    "item_similarities = make_item_similarities(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Prediction\n",
    "\n",
    "The prediction step, again, is purely algebra.\n",
    "\n",
    "We compute the predicted ratings as a weighted average of the ratings of other users, or items.\n",
    "\n",
    "## 3.1 User-user CF\n",
    "\n",
    "Given the subset of users $U_i$ that rated the item $i \\in I$, with cardinality $|U_i| = K$, and $v \\in U_i$:\n",
    "\n",
    "$$\\hat{r}_{u, i} = \\frac{\\sum\\limits_{k=1}^{K} sim(u, v_k)r_{v_k, i}}{\\sum\\limits_{k=1}^{K} |sim(u, v_k)|}$$\n",
    "\n",
    "The predicted rating $\\hat{r}_{u, i}$ is the average of all ratings by other users that rated the item, weighted by user similarity.\n",
    "\n",
    "We can implement the weighted sum as a dot-product between the similarities $\\in \\mathbb{R}^{\\space m \\space \\times \\space m}$ and ratings $\\in \\mathbb{R}^{\\space m \\space \\times \\space n}$.\n",
    "\n",
    "Then, we use broadcasting to normalize the results using the sum of weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_user_predictions(similarities, R):\n",
    "    weighted_sum = np.dot(similarities, R)\n",
    "    sum_of_weights = np.abs(similarities).sum(axis=1)\n",
    "    \n",
    "    return np.array(weighted_sum / sum_of_weights)\n",
    "\n",
    " \n",
    "L_user = make_user_predictions(user_similarities, R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Item-item CF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $I_u$, the set of items rated by the user $u \\in U$, with cardinality $|I_u| = W$, and $j \\in I_u$.\n",
    "\n",
    "$$\\hat{r}_{u, i} = \\frac{\\sum\\limits_{w=1}^{W} sim(i, j_k)r_{u, j_k}}{\\sum\\limits_{k=1}^{K} |sim(i, j_k)|}$$\n",
    "\n",
    "The predicted rating $\\hat{r}_{u, i}$ is the average of all ratings by the same user to other items, weighted by item similarity.\n",
    "\n",
    "Again, we can implement the weighted sum as a dot-product between the ratings $\\in \\mathbb{R}^{\\space m \\space \\times \\space n}$ and the similarities $\\in \\mathbb{R}^{\\space n \\space \\times \\space n}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_item_predictions(similarities, R):\n",
    "    weighted_sum = np.dot(R, similarities)\n",
    "    sum_of_weights = np.abs(similarities).sum(axis=0)\n",
    "    \n",
    "    pred = np.array(weighted_sum / sum_of_weights)\n",
    "    pred[np.isnan(pred)] = 0\n",
    "    \n",
    "    return pred\n",
    "\n",
    " \n",
    "L_item = make_item_predictions(item_similarities, R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We replace missing values, resulting from the division by zero, with zero, to avoid problems downstream.\n",
    "\n",
    "And, just like that, we were able to perform predictions using just ratings and linear algebra. Now, onto the usual filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Filtering\n",
    "\n",
    "## 4.1 Removing Rated Items\n",
    "\n",
    "We remove previously rated items, like we did in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_rated_items(R, L):\n",
    "    L = L.copy()\n",
    "    L[R.nonzero()] = -1\n",
    "    return L\n",
    "\n",
    "\n",
    "L_user_ = mask_rated_items(L_user)\n",
    "L_item_ = mask_rated_items(L_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Best-item\n",
    "\n",
    "We use `argmax` to get the best item, please note that we are not working with sparse matrices for a while now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_item(L):\n",
    "    return L.argmax(axis=1) + 1\n",
    "\n",
    "\n",
    "best_item_user = get_best_item(L_user_)\n",
    "best_item_item = get_best_item(L_item_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Top-N\n",
    "\n",
    "Moreover, as usual, we use `argsort` on the array to retrieve the top-*N* list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(L, n):\n",
    "    return np.negative(L).argsort()[:, :n] + 1\n",
    "\n",
    "\n",
    "top_5_user = get_top_n(L_user_, 5)\n",
    "top_5_item = get_top_n(L_item_, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
