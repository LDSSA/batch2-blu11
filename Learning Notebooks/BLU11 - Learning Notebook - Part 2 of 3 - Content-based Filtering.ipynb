{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLU11 - Learning Notebook - Part 2 of 3 - Content-based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Memory-based Recommender System (RS)\n",
    "\n",
    "Memory-based RS use previous interactions to predict the interest of a given user in a particular item, in a personalized way.\n",
    "\n",
    "This approach differs from the typical model-based techniques, as we are not learning any parameters.\n",
    "\n",
    "The primary assumption is that user preferences are stable over time and, thus, the user likes are similar to those he liked in the past.\n",
    "\n",
    "In this notebook, we start by analyzing a particular type of memory-based recommendations, content-based.\n",
    "\n",
    "Finally, in the next notebook, we learn about collaborative filtering, the most widely adopted technique when building RS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Content-based RS\n",
    "\n",
    "Content-based RS are among the most complex RS concerning the components, involving:\n",
    "1. The base model, comprised of **Users** and **Items** (i.e., a Community), plus **Ratings**\n",
    "1. **Item Profiles**, describing the content of the item according to relevant attributes\n",
    "2. A **User Model**, user preferences regarding item attributes, so both entities are in the same vector space.\n",
    "\n",
    "The diagram below offers a sneak peek at how they look like, in the end.\n",
    "\n",
    "![recommender_systems_framework](../media/recommender_systems_framework_content_based.png)\n",
    "\n",
    "We derive all of these different components employing a series of sequential steps, in what we call a pipeline.\n",
    "\n",
    "Content-based pipelines are somewhat standardized, following the diagram below.\n",
    "\n",
    "![content_based_filtering](../media/content_based_filtering.png)\n",
    "\n",
    "We zoom in into each one of the individual components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Feature Extraction\n",
    "\n",
    "The first step in most pipelines is **feature extraction**, in which we mine item metadata to generate descriptive representations.\n",
    "\n",
    "Such metadata can be either structured or unstructured. For unstructured data, the feature extraction process is akin to NLP.\n",
    "\n",
    "## 3.1 TF-IDF Weighting\n",
    "\n",
    "If we denote by $f_{i, t}$ the raw frequency counts, i.e., the number of times that $t$ occurs in $i$:\n",
    "\n",
    "$$tf(i, t) = f_{i, t}$$\n",
    "\n",
    "$$idf(t, I) = log\\frac{n}{|i \\in I : t \\in i|}$$\n",
    "\n",
    "With $|i \\in I : t \\in i|$ is the number of documents that contain $t$, i.e., $f_{i, t} > 0$.\n",
    "\n",
    "Therefore, we can compute the TF-IDF (Term's Frequency - Inverse Document Frequency) value for a given term $t$ for an item $i$, in corpus $I$, as:\n",
    "\n",
    "$$tfidf(i, t, I) = tf(i, t) \\times idf(t, I)$$\n",
    "\n",
    "## 3.2 Item Profiles\n",
    "\n",
    "Let $T$ be a size-$k$ set of extracted terms or concepts and $I$ a size-$n$ item-set, items profiles are represented as:\n",
    "\n",
    "$$\\begin{bmatrix}tfidf(i_0, t_0) & tfidf(i_0, t_1) & ... & tfidf(i_0, t_k) \\\\ tfidf(i_1, t_0) & tfidf(i_1, t_1) & ... & tfidf(i_1, t_k) \\\\ ...  & ... & ... & ...\\\\ tfidf(i_n, t_0) & tfidf(i_n, t_1) & ... & tfidf(i_n, t_k)\\end{bmatrix}$$\n",
    "\n",
    "TF-IDF assumes that rare terms have more descriptive power, even if rarity doesn't imply more significance in all contexts.\n",
    "\n",
    "The item profiles matrix $P \\in \\mathbb{R}^{\\space n \\space \\times \\space k}$ has items as row-vectors in a vector space in which there is one column for each term.\n",
    "\n",
    "## 3.3 Advanced Techniques\n",
    "\n",
    "Advanced techniques, which we don't cover, allow us for mining the items themselves (e.g., text, images, audio) for features.\n",
    "\n",
    "An example would be convolutional neural networks, commonly applied to analyzing visual imagery.\n",
    "\n",
    "## 3.4 Item Profiles in Practice\n",
    "\n",
    "In this example, we use the [MovieLens Dataset](https://grouplens.org/datasets/movielens/). From the source:\n",
    "\n",
    "> The small dataset 100,000 movie ratings and 1,300 tag applications applied to 9,000 movies by 700 users.\n",
    "\n",
    "Although we are using the small version of the dataset, a full version with 26,000,000 ratings is [available from the website](https://grouplens.org/datasets/movielens/latest/).\n",
    "\n",
    "The ratings are on a 1-5 scale, and both User and Item IDs start at 1.\n",
    "\n",
    "### 3.4.1 Read the Data\n",
    "\n",
    "We stored some data preprocessed data under `../data/processed/genres.csv`.\n",
    "\n",
    "This table contains previously extracted genre information about some of the uses, which we need to store as item profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_genres():\n",
    "    \n",
    "    path = os.path.join('..', 'data', 'processed', 'genres.csv')\n",
    "    \n",
    "    movies = np.genfromtxt(path, dtype='int', skip_header=True, usecols=[0], delimiter=',')\n",
    "    genres = np.genfromtxt(path, dtype='object', skip_header=True, usecols=[1], delimiter=',')\n",
    "    \n",
    "    return movies, genres\n",
    "\n",
    "\n",
    "movies, genres = read_genres()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading the data, creating two different arrays: `movies` and `genres`, which are the same size and aligned.\n",
    "\n",
    "(Also, we use `os.path.join` to ensure compatibility with different Operating Systems.)\n",
    "\n",
    "### 3.4.2 Sparse Matrix Structure\n",
    "\n",
    "Although we don't have loads of data (as of yet), as we've seen the best practice in RS is to work with sparse matrices. Let's build one.\n",
    "\n",
    "Since `genres` contains strings, let's use `np.unique` to transform it into positions that we can use as column indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_unique, genre_pos, genre_count = np.unique(genres, return_inverse=True, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note the new parameter `return_counts=True`, which we didn't use before.\n",
    "\n",
    "It contains how many movies contain a genre, in the entire corpus. We use later on when computing TF-IDF values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Action' 1545\n",
      "b'Adventure' 1117\n",
      "b'Animation' 447\n",
      "b'Children' 583\n",
      "b'Comedy' 3315\n",
      "b'Crime' 1100\n",
      "b'Documentary' 495\n",
      "b'Drama' 4365\n",
      "b'Fantasy' 654\n",
      "b'Film-Noir' 133\n",
      "b'Horror' 877\n",
      "b'IMAX' 153\n",
      "b'Musical' 394\n",
      "b'Mystery' 543\n",
      "b'Romance' 1545\n",
      "b'Sci-Fi' 792\n",
      "b'Thriller' 1729\n",
      "b'War' 367\n",
      "b'Western' 168\n"
     ]
    }
   ],
   "source": [
    "for k, i in zip(genre_unique, genre_count):\n",
    "    print(k, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to build a `COO` matrix, as suggested in the previous notebook. Then, we convert it to `CSR`.\n",
    "\n",
    "Thus, we need the aligned row (movies) and column (genres) arrays, containing the coordinates of non-zero values.\n",
    "\n",
    "We subtract one to the movie ID, because Python arrays start at 0, while IDs start at 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = genre_pos\n",
    "rows = movies - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the shape of the matrix, we use the maximum movie ID is our corpus and the maximum genre position.\n",
    "\n",
    "This way, we ensure that all possible combinations of movie-genre coordinate pairs are compatible with our matrix structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = rows.max() + 1\n",
    "ncols = cols.max() + 1\n",
    "shape = (nrows, ncols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3 Sparse Matrix Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the values, we know that each genre either is present or not present for a movie.\n",
    "\n",
    "Therefore, the frequency count is binary, either 1 or 0. \n",
    "\n",
    "We only need to compute the IDF, to adjust the weight of each genre to promote more obscure ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Action' 4.6707942827237385\n",
      "b'Adventure' 4.995171672986383\n",
      "b'Animation' 5.911014877442015\n",
      "b'Children' 5.645386285705092\n",
      "b'Comedy' 3.907360569435621\n",
      "b'Crime' 5.010508013269122\n",
      "b'Documentary' 5.809015709486894\n",
      "b'Drama' 3.6322000037818816\n",
      "b'Fantasy' 5.530466120598385\n",
      "b'Film-Noir' 7.12322434383383\n",
      "b'Horror' 5.237066479683401\n",
      "b'IMAX' 6.983135550663149\n",
      "b'Musical' 6.03722256275765\n",
      "b'Mystery' 5.716464152121649\n",
      "b'Romance' 4.6707942827237385\n",
      "b'Sci-Fi' 5.339012080241158\n",
      "b'Thriller' 4.558274986372294\n",
      "b'War' 6.108211624001014\n",
      "b'Western' 6.889609492652325\n"
     ]
    }
   ],
   "source": [
    "idf = np.log(np.divide(nrows, genre_count))\n",
    "\n",
    "for k, i in zip(genre_unique, idf):\n",
    "    print(k ,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By applying a vectorized version of the formula above, we succeeded in giving more weight to less familiar genres.\n",
    "\n",
    "Finally, we generate the data by retrieving the corresponding IDF values for each position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = idf[genre_pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.4 Building a Wrapper\n",
    "\n",
    "It's useful (and good practice) to encapsulate all of this logic into a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<164979x19 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 20322 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_item_profiles(movies, genres):\n",
    "    \n",
    "    genre_unique, genre_pos, genre_count = np.unique(genres, return_inverse=True, return_counts=True)\n",
    "    \n",
    "    cols = genre_pos\n",
    "    rows = movies - 1\n",
    "    \n",
    "    nrows = rows.max() + 1\n",
    "    ncols = cols.max() + 1\n",
    "    shape = (nrows, ncols)\n",
    "    \n",
    "    idf = np.log(np.divide(nrows, genre_count))\n",
    "    \n",
    "    data = idf[genre_pos]\n",
    "    \n",
    "    coo = coo_matrix((data, (rows, cols)), shape=shape)\n",
    "    \n",
    "    return coo.tocsr()\n",
    "\n",
    "\n",
    "P = make_item_profiles(movies, genres)\n",
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the Item Profiles in a sparse matrix, in the convenient shape, the feature extraction step is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Profile Learner\n",
    "\n",
    "Once we have the items as vectors in the new feature space, we need to **learn user profiles**, so that we can represent users in the same space.\n",
    "\n",
    "The purpose of the Profile Learner is to uncover user preferences that, in short, tell us what the preference of a user for an attribute is.\n",
    "\n",
    "Borrowing from Economics, we denote preference by the utility, i.e., what the utility of $t$ to the user $u$.\n",
    "\n",
    "$$\\begin{bmatrix}util(u_0, t_0) & util(u_0, t_1) & ... & util(u_0, t_k) \\\\ util(u_1, t_0) & util(u_1, t_1) & ... & util(u_1, t_k) \\\\ ...  & ... & ... & ...\\\\ util(u_n, t_0) & util(u_n, t_1) & ... & util(u_n, t_k)\\end{bmatrix}$$\n",
    "\n",
    "The User Model matrix $M \\in \\mathbb{R}^{\\space n \\space \\times \\space k}$ has users as row-vectors, but the same number of columns as the Item Profiles matrix $P$.\n",
    "\n",
    "In our example, the attributes are movie genres.\n",
    "\n",
    "## 4.1 Uncovering Preferences\n",
    " \n",
    "However, how can we infer user utility? We don't have this data. (Explicitly, that is.)\n",
    "\n",
    "We combine the Ratings (that, remember, establish relationships between users to items), with the Item Profiles. \n",
    "\n",
    "### 4.1.1 Read the Ratings Data\n",
    "\n",
    "We use the same approach we did for `genres.csv`, by creating three distinct arrays:\n",
    "* Users\n",
    "* Movies\n",
    "* Ratings.\n",
    "\n",
    "This format is convenient to build `COO` sparse matrices, which is what we want to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ratings():\n",
    "    path = os.path.join('..', 'data', 'ml-latest-small', 'ratings.csv')\n",
    "    \n",
    "    users = np.genfromtxt(path, dtype='int', skip_header=True, usecols=[0], delimiter=',')\n",
    "    movies = np.genfromtxt(path, dtype='int', skip_header=True, usecols=[1], delimiter=',')\n",
    "    ratings = np.genfromtxt(path, skip_header=True, usecols=[2], delimiter=',')\n",
    "    \n",
    "    return users, movies, ratings\n",
    "\n",
    "\n",
    "users, movies, ratings = read_ratings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Ratings Matrix\n",
    "\n",
    "We don't detail the construction of the ratings matrix because we use the same logic as above.\n",
    "\n",
    "The main different is that we use `max(P.shape[0], cols.max() + 1)` for the number of rows columns in the matrix structure.\n",
    "\n",
    "Since we have a sample of data and we are not sure about which table has the highest possible movie ID value in the dataset, we play it safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ratings(users, movies, ratings, P):\n",
    "    \n",
    "    cols = movies - 1\n",
    "    rows = users - 1\n",
    "    \n",
    "    nrows = rows.max() + 1\n",
    "    ncols = max(P.shape[0], cols.max() + 1)\n",
    "    shape = (nrows, ncols)\n",
    "    \n",
    "    data = ratings\n",
    "    \n",
    "    coo = coo_matrix((data, (rows, cols)), shape=shape)\n",
    "    \n",
    "    return coo.tocsr()\n",
    "\n",
    "    \n",
    "R = make_ratings(users, movies, ratings, P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that, since we preprocessed the data, we know that the highest value is, in fact, in the Item Profiles matrix `P`.\n",
    "\n",
    "If we did not know this, i.e., in the real world, the `max` should be included in both `make` functions.\n",
    "\n",
    "Why? To make sure that we consider the highest value in both sets combined when building the structure of our matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 User Model\n",
    "\n",
    "Wait, but why indeed? Why are shapes so important, at all times?\n",
    "\n",
    "Because the product of $R \\in \\mathbb{R}^{\\space m \\space \\times \\space n}$ and $P \\in \\mathbb{R}^{\\space n \\space \\times \\space k}$ is the matrix $M = RP \\in \\mathbb{R}^{\\space m \\space \\times \\space k}$, where:\n",
    "\n",
    "$$M_{ij} = \\sum\\limits_{k=1}^n R_{ik}P_{kj}$$\n",
    "\n",
    "Wait. Do you see it? $M$ is a $m$ by $k$ matrix, i.e., users by attributes. However, can it be?\n",
    "\n",
    "Let's give it some thought, armed with some linear algebra.\n",
    "\n",
    "$$M = RP = \\begin{bmatrix}r_0^Tp_0 & r_0^Tp_1 & ... & r_0^Tp_k \\\\ r_1^Tp_0 & r_1^Tp_1 & ... & r_1^Tp_k \\\\ ...  & ... & ... & ...\\\\ r_m^Tp_0 & a_m^Tp_1 & ... & r_m^Tp_k\\end{bmatrix}$$\n",
    "\n",
    "Again, since $R \\in \\mathbb{R}^{\\space m \\space \\times \\space n}$ and $P \\in \\mathbb{R}^{\\space n \\space \\times \\space k}$, we have $r_i \\in \\mathbb{R}^n$ and $p_j \\in \\mathbb{R}^n$. \n",
    "\n",
    "Each element is a dot-product of user ratings by item-attributes. (Take the time to do the math by hand if needed. We know we did.)\n",
    "\n",
    "So we can uncover user preferences with the wicked sorcery of linear algebra!\n",
    "\n",
    "Alternatively, we can directly call `np.dot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.dot(R, P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we encapsulate it into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_user_profile(R, P):\n",
    "    return np.dot(R, P)\n",
    "\n",
    "\n",
    "M = learn_user_profile(R, P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Prediction\n",
    "\n",
    "The **Prediction** step consist of comparing user and item vector, which are now all in the same vector space.\n",
    "\n",
    "Items whose profile is the most similar to that of a given user, have the highest score.\n",
    "\n",
    "At this point, we are asking (somewhat philosophical, we must say): what does most similar mean?\n",
    "\n",
    "Let's look into some alternative distance and similarity metrics used in RS.\n",
    "\n",
    "## 5.1 Distance Metrics\n",
    "\n",
    "### 5.1.1 Euclidean Distance\n",
    "\n",
    "The most straightforward distance measure is the Euclidean distance. Logically, the higher the distance, the lesser the similarity.\n",
    "\n",
    "At the core, we subtract both vectors element-wise and, given vectors $x, y \\in \\mathbb{R}^n$:\n",
    "\n",
    "$$d(x, y) = \\sqrt{\\sum\\limits_{k=1}^n (x_k - y_k)^2}$$\n",
    "\n",
    "Often, this is the default method for many distance-based machine learning algorithms, namely *k*NN.\n",
    "\n",
    "There are many problems with the Euclidean distance, for example, if the elements are in different scales (i.e., non-standardized).\n",
    "\n",
    "### 5.1.2 Dot-product Similarity\n",
    "\n",
    "The dot product can also be seen as a measure of similarity (the higher, the better). Take again $x, y \\in \\mathbb{R}^n$:\n",
    "\n",
    "$$d(x, y) = \\sum\\limits_{i=1}^n x_{i}y_{i}$$\n",
    "\n",
    "Simplistically, a large dot product indicates that elements in both vectors have significant values in the same positions.\n",
    "\n",
    "But this product considers the magnitude of the vectors, i.e., their length, as given by the Euclidean norm:\n",
    "\n",
    "$$||x|| = \\sqrt{x_1^2 + x_2^2 + ... + x_n^2}$$\n",
    "\n",
    "So, we penalize two relatively identical vectors because one of them has a lower magnitude. Also, we favor vectors with high magnitude.\n",
    "\n",
    "Now, both the Euclidean distance and the dot-product consider the magnitude though and they should. What can we do?\n",
    "\n",
    "### 5.1.3 Cosine Similarity\n",
    "\n",
    "As usual, there is a better way, and that is using just the direction, i.e., the angle, of the vector, ignoring their magnitude.\n",
    "\n",
    "Hence, the cosine similarity, which measures differences in orientation by using the cosine of the angle between two vectors.\n",
    "\n",
    "$$cos(x, y) = \\frac{xy}{||x||||y||}$$\n",
    "\n",
    "The cosine similarity is, simply put, the dot-product normalized. Given a vector $a \\in \\mathbb{R}^n$:\n",
    "\n",
    "$$\\hat{a} = \\frac{a}{||a||}$$\n",
    "\n",
    "We return a vector $\\hat{a}$ with the same orientation as the original vector but with length one, i.e., a unit-vector. \n",
    "\n",
    "Additionally, the cosine similarity efficiently considers only the non-zero dimensions in sparse vectors.\n",
    "\n",
    "### 5.1.4 Pearson Correlation\n",
    "\n",
    "Finally, we measure the similarity as the correlation, i.e., the linear relationship between the vectors.\n",
    "\n",
    "The Pearson correlation is the most commonly used coefficient, as:\n",
    "\n",
    "$$Pearson(x, y) = \\frac{cov(x, y)}{\\sigma_x \\sigma_y}$$\n",
    "\n",
    "Where $cov$ is the covariance and $\\sigma_x$ and $\\sigma_y$ the standard deviations of $x$ and $y$, respectively.\n",
    "\n",
    "### 5.1.5 Making Predictions\n",
    "\n",
    "Most RS software uses the cosine similarity to make predictions, based on arithmetic operations.\n",
    "\n",
    "We use the method implementation from `sklearn` but others are available. \n",
    "\n",
    "(Again, note that we are not learning any parameters.)\n",
    "\n",
    "By computing the distance between the User Models, as $M$, and the Item Profiles, $P$, we obtain a matrix containing predicted ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = cosine_similarity(M, P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix $L$, which contains how similar is each user to each item, has the same shape as our original ratings matrix $R$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R.shape == L.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's encapsulate this logic into a self-explanatory function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(M, P):\n",
    "    return cosine_similarity(M, P)\n",
    "\n",
    "\n",
    "L = make_predictions(M , P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can predict the utility of a given item to a given user, in a personalized way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Filtering Component\n",
    "\n",
    "The filtering component, which is very similar to what we did for non-personalized RS, does two main tasks:\n",
    "* Remove previously rated items (mask with non-zeros from ratings matrix)\n",
    "* Provides a list with the top-*N* most recommended items for the user.\n",
    "\n",
    "## 6.1 Removing Rated Items\n",
    "\n",
    "We can use a mask to select the previously rated items and replace their predictions with minus one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.56411863,  0.50560889,  0.19625371, ...,  0.20389081,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.46780893,  0.33612265,  0.5464153 , ...,  0.39789046,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.39299699,  0.27181483,  0.43588561, ...,  0.37690207,\n",
       "         0.        ,  0.09482589],\n",
       "       ...,\n",
       "       [ 0.27850585,  0.14707266,  0.42388622, ...,  0.48424713,\n",
       "         0.        ,  0.        ],\n",
       "       [-1.        ,  0.24911524,  0.38030068, ...,  0.24945139,\n",
       "         0.        ,  0.        ],\n",
       "       [-1.        ,  0.48489548,  0.49310015, ...,  0.46084461,\n",
       "         0.        ,  0.04315785]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mask_rated_items(L, R):\n",
    "    L = L.copy()\n",
    "    L[R.nonzero()] = -1\n",
    "    return L\n",
    "\n",
    "\n",
    "L_ = mask_rated_items(L, R)\n",
    "L_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Best-item\n",
    "\n",
    "Without surprises, we use `argmax` to get the best item.\n",
    "\n",
    "Don't forget that we need to add 1 to convert it back to the original movie ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  8361,   4956,   4956,    546,      4,   4818,  40339,    459,\n",
       "         1912,  55116,   6016,   4956, 134853,  40339,   4956,   1912,\n",
       "         2058,   4956,   4956,  42015,   4956,  27032,   1912,   6016,\n",
       "           20,     20,   1912,   1912,   6395,   3893,     20,  55116,\n",
       "           72,   8361,   1912,   1912,  26093,     14,   7235,   8361,\n",
       "         8361,   8361,      4,   4956,    496,   6990,   6016, 161594,\n",
       "         2890,   4956,   5018,      4,  55116,    459,  55116,   4956,\n",
       "         4956,  55116,   6990,   1396,  26093,   8361,   4956,    459,\n",
       "           72,    519,   4956,  45672,  26093,   4956,   2940,   8361,\n",
       "        55116,   8361,  27032,    319,  27032,  58025,   8361,   4719,\n",
       "           14,   4956,     72,  55116,  55116,      4,   4956,   4956,\n",
       "        27032,    459,  27032,   4956,  42015,    459,  56069,   1783,\n",
       "         4956,  42015,   4956,   4956])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_best_item(L):\n",
    "    return L.argmax(axis=1) + 1\n",
    "\n",
    "\n",
    "best_item = get_best_item(L_)\n",
    "best_item[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Top-N\n",
    "\n",
    "And we use `argsort` for the top-*N*, as we did in the previous learning unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  8361,  48774, 117529,  91500,  58025],\n",
       "       [  4956,    496,   5666,  42015,  74438],\n",
       "       [  4956,   1432,   7007,   5027,     20],\n",
       "       ...,\n",
       "       [ 55116,  70728,   7235,   4956,    145],\n",
       "       [  3893,   1912,    459,   3598,   3800],\n",
       "       [  4956,  42015,  74438,    970,   6990]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_top_n(L, n):\n",
    "    return np.negative(L).argsort()[:, :n] + 1\n",
    "\n",
    "\n",
    "top_5 = get_top_n(L_, 5)\n",
    "top_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
